{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN4.ipynb","provenance":[{"file_id":"1Z0PG49KTlxdas36FUI5IFhivsNpcInMi","timestamp":1575910870094},{"file_id":"1v8Ea5B6AkM6W_dAnpZs9aElz0t2aG4ZT","timestamp":1575885375137},{"file_id":"1wXiHn4tF4BLe6G_yXbG1-5vIUwuO88Oc","timestamp":1575764161775},{"file_id":"19WCV1NOgu2HzdQZunNHl0x86TgY2I_sT","timestamp":1575063257833}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zbo7SSyofl0r"},"outputs":[],"source":["To change/test below:\n","every new specification new workplace to be saved!!\n","reduce (& vary) epsilon decay. should not be down to 0.01 too quickly\n","vary dimensions of neural network (line 68)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"96E89gDOhnF8"},"outputs":[],"source":["<h4>Specifications of this dqn work space:</h4> \n","<li>Training: 5000</li>\n","<li>Neural dimensions 1,2 : 256, 256</li>\n","<li>eps_dec= 1/5000</li>\n","<li>alpha = 0.001 </li>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"A0YmfPF-feAI"},"outputs":[],"source":["Goal for win statistic set at maximum tile >= 256"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":100},"colab_type":"code","executionInfo":{"elapsed":7828,"status":"ok","timestamp":1576006457952,"user":{"displayName":"matteo brändli","photoUrl":"","userId":"04935261826176898211"},"user_tz":-60},"id":"nZV4E_32VmVK","outputId":"b9d3ef80-d0fb-4a65-a160-cd7661acc34d"},"outputs":[],"source":["from keras.layers import Dense, Activation\n","from keras.models import Sequential, load_model\n","from keras.optimizers import Adam\n","import numpy as np \n","\n","#numpy_transformer changes the structure of the matrix into an array, so that we can easily use the code\n","def numpy_transformer(matrix):\n","    lst = []\n","    for i in range(len(matrix)):\n","        for j in range(len(matrix[0])):\n","            lst.append(int(matrix[i][j]))\n","    return lst\n","\n","\n","class ReplayBuffer(object):\n","    def __init__(self, max_size, input_shape, n_action, discrete = False):\n","        self.mem_size = max_size\n","        self.input_shape = input_shape\n","        self.discrete = discrete\n","        self.mem_cntr = 0\n","        self. state_memory = np.zeros((self.mem_size, input_shape))\n","        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n","        dtype = np.int8 if self.discrete else np.float32\n","        self.action_memory = np.zeros((self.mem_size, n_action), dtype = dtype)\n","        self.reward_memory = np.zeros((self.mem_size))\n","        self.terminal_memory = np.zeros(self.mem_size, dtype = np.float32)\n","        \n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_cntr % self.mem_size\n","        self.state_memory[index] = state\n","        self.new_state_memory[index] = state_\n","        self.reward_memory[index] = reward\n","        self.terminal_memory[index] = 1 - int(done)\n","        if self.discrete:\n","            actions = np.zeros(self.action_memory.shape[1])\n","            actions[action] = 1.0\n","            self.action_memory[index] = actions\n","        else:\n","            self.action_memory[index] = action\n","        self.mem_cntr+=1\n","\n","    def sample_buffer(self, batch_size):\n","        max_mem = min(self.mem_cntr, self.mem_size)\n","        batch = np.random.choice(max_mem, batch_size)\n","\n","        states = self.state_memory[batch]\n","        states_ = self.new_state_memory[batch]\n","        rewards = self.reward_memory[batch]\n","        actions = self.action_memory[batch]\n","        terminal = self.terminal_memory[batch]\n","\n","        return states, actions, rewards, states_ , terminal\n","\n","def build_dqn(learning_rate, n_actions, input_dims, fc1_dims,fc2_dims):\n","    model = Sequential([Dense(fc1_dims, input_shape = (input_dims, )),Activation('relu'),Dense(fc2_dims),Activation('relu'),Dense(n_actions)])\n","    \n","    model.compile(optimizer = Adam(learning_rate), loss ='mse')\n","\n","    return model\n","\n","#lower \"espilon_dec\" to be chosen. otherwise, after approx. 100 episodes epsilon already down 0.01.2048 too complex (many many states) for this to be sufficient\n","class Agent(object):\n","    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, input_dims, epsilon_dec = 1/5000, epsilon_end = 0.01,mem_size = 1_000_000, fname = 'dqn_model.h5'):\n","        self.action_space = [i for i in range(n_actions)]\n","        self.n_actions = n_actions\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.epsilon_dec = epsilon_dec\n","        self.epsilon_min = epsilon_end\n","        self.batch_size = batch_size\n","        self.model_file = fname\n","        self.penalty = 0\n","\n","        self.memory = ReplayBuffer(mem_size, input_dims, n_actions, discrete= True)\n","\n","#change the last two integers in function below to change architecture of neural network\n","        self.q_eval = build_dqn(alpha, n_actions,input_dims,256,256)\n","\n","    def remember(self, state, action, reward, new_state, done):\n","        self.memory.store_transition(state,action,reward, new_state, done)\n","\n","    def choose_action(self, observation):\n","        if np.random.random() < self.epsilon:\n","            action = np.random.choice(self.action_space)\n","        else:\n","            state = np.array([observation], copy=False, dtype=np.float32)\n","            actions = self.q_eval.predict(state)\n","            action = np.argmax(actions)\n","\n","        return action\n","\n","    def choose_action_expl(self, observation,env):\n","          state = np.array([observation], copy=False, dtype=np.float32)\n","          actions = self.q_eval.predict(state)\n","          action = np.argmax(actions)\n","          observation_ , reward, done, info = env.step(action)\n","          observation2 = numpy_transformer(observation_)\n","\n","          while observation == observation2:\n","              action = np.random.choice(self.action_space)\n","              self.penalty+=1\n","              observation_ , reward, done, info = env.step(action)\n","              observation2 = numpy_transformer(observation_)\n","          return observation2, reward, done, info, action\n","\n","    def full_explotation(self):\n","        self.epsilon = 0\n","        self.epsilon_min = 0\n","        self.epsilon_dec = 0\n","\n","\n","    def learn(self):\n","        if self.memory.mem_cntr < self.batch_size:\n","            return\n","        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n","\n","        action_values = np.array(self.action_space, dtype = np.int8)\n","        action_indices = np.dot(action, action_values)\n","\n","        q_eval = self.q_eval.predict(state)\n","        q_next = self.q_eval.predict(new_state)\n","\n","        q_target = q_eval.copy()\n","\n","        batch_index = np.arange(self.batch_size, dtype = np.int32)\n","\n","        q_target[batch_index, action_indices] = reward + self.gamma * np.max(q_next, axis = 1)*done\n","\n","        _ = self.q_eval.fit(state, q_target, verbose = 0)\n","\n","        self.epsilon = self.epsilon * self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n","\n","    def save_model(self):\n","        self.q_eval.save(self.model_file)\n","\n","    def load_model(self):\n","        self.q_eval = load_model(self.model_file)\n","\n","    def get_penalties(self):\n","        return self.penalty"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"colab_type":"code","executionInfo":{"elapsed":11475,"status":"error","timestamp":1576006461624,"user":{"displayName":"matteo brändli","photoUrl":"","userId":"04935261826176898211"},"user_tz":-60},"id":"gEV1qhPtbDmc","outputId":"c6bee7dd-edb0-4e5b-d56e-0a5944db51c8"},"outputs":[],"source":["#from dqn2 import Agent\n","import numpy as np\n","import gym\n","import gym_2048\n","\n","\n","if __name__ == '__main__':\n","    env = gym.make('2048-v0')\n","    n_games = 5000\n","    BATCH_NUMBER = 100\n","    BATCH_SIZE = n_games/BATCH_NUMBER\n","    agent = Agent(gamma = 0.99, epsilon = 1.0 , alpha = 0.001, input_dims = 16, n_actions = 4, mem_size = 10_000_000, batch_size = 64, epsilon_end=0.01)\n","\n","    scores = []\n","    eps_history = []\n","    win_rates = []\n","    #counts how many times we have won the game\n","    total_score = 0\n","    won = 0\n","    max_value = 0\n","    GOAL = 256\n","    max_values = []\n","    max_tile_episode = [] #to track max_tile in each episode\n","    max_tile_sum = 0 #to calculate avg max tile in each batch\n","    avg_max_tile = []\n","    \n","\n","    for i in range(n_games):\n","        done = False\n","        #checks if we have won the game, initially set to false\n","        won_mem = False\n","        observation = env.reset()\n","        observation_trans = list(numpy_transformer(observation))\n","        while not done:\n","            action = agent.choose_action(observation_trans)\n","            observation_ , reward, done, info = env.step(action)\n","            total_score_test += reward\n","            observation_trans_ = numpy_transformer(observation_)\n","            agent.remember(observation_trans, action, reward, observation_trans_, done)\n","            observation_trans = observation_trans_\n","            agent.learn()\n","            # checks if the array has a value, which is bigger than 2048\n","            if max(observation_trans_)==GOAL:\n","                won_mem = True\n","            if max(observation_trans_)>max_value_test:\n","                max_value_test = max(observation_trans_)\n","\n","\n","        max_tile_episode.append(max(observation_trans))\n","        max_tile_sum += max(observation_trans)\n","\n","        eps_history.append(agent.epsilon)\n","        #updates the number of games, which we have won\n","        if won_mem == True:\n","            won+=1\n","\n","        \n","\n","        # prints out every 50 episode, how many games we have won from how many episodes and creates a \n","        if (i+1)%BATCH_SIZE == 0:\n","            print(\"You have won \" + str(won) + \" episodes from \" + str(int(i+1)) + \" episodes\")\n","            print(\"The maximal value is: \" + str(max_value) + \".\")\n","            scores.append(total_score/BATCH_SIZE)\n","            win_rates.append(won/BATCH_SIZE)\n","            max_values.append(max_value)\n","            avg_max_tile.append(max_tile_sum/BATCH_SIZE)\n","            total_score = 0\n","            max_value = 0\n","            won = 0\n","            max_tile_sum = 0\n","\n"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"MxlvD0NxWJHY"},"outputs":[],"source":["x = [j for j in range(1,101)]"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"JmOMg4PmkZGg"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# get coeffs of linear fit\n","slope, intercept, r_value, p_value, std_err = stats.linregress(x,win_rates)\n","\n","# use line_kws to set line label for legend\n","ax = sns.regplot(x=x, y=win_rates, color='b', \n"," line_kws={'label':\"y={0:.8f}x+{1:.8f}\".format(slope,intercept)})\n","\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Games won\")\n","plt.title(\"Win Statistic with 256\")\n","\n","# plot legend\n","ax.legend()\n","\n","plt.show()"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"Q9F8eXxPkqUM"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# get coeffs of linear fit\n","slope, intercept, r_value, p_value, std_err = stats.linregress(x,scores)\n","\n","# use line_kws to set line label for legend\n","ax = sns.regplot(x=x, y=scores, color='b', \n"," line_kws={'label':\"y={0:.3f}x+{1:.3f}\".format(slope,intercept)})\n","\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"reward per batch\")\n","plt.title(\"Reward-statistic\")\n","\n","# plot legend\n","ax.legend()\n","\n","plt.show()"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"OimTXQ3WpPXl"},"outputs":[],"source":["\n","plt.scatter(x,max_values, label = 'skitscat', color = 'k', s=25 , marker = 'o')\n","plt.xlabel('batches')\n","plt.ylabel('maximal values')\n","plt.title('Maximal value statistic')"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"qyQ3tadULwKM"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# get coeffs of linear fit\n","slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_max_tile)\n","\n","# use line_kws to set line label for legend\n","ax = sns.regplot(x=x, y=avg_max_tile, color='b', \n"," line_kws={'label':\"y={0:.3f}x+{1:.3f}\".format(slope,intercept)})\n","\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"avg_max_tile\")\n","plt.title(\"Average max Tile\")\n","\n","# plot legend\n","ax.legend()\n","\n","plt.show()"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"jIiNI56GMOXX"},"outputs":[],"source":["xx = [j for j in range(1,len(max_tile_episode)+1)]\n","plt.scatter(xx,max_tile_episode, label = 'skitscat', color = 'k', s=25 , marker = 'o')\n","plt.xlabel('episode')\n","plt.ylabel('maximal tile')\n","plt.title('Maximal Value per episode statistic')"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"kBB98YoWPRw6"},"outputs":[],"source":["if __name__ == '__main__':\n","    env = gym.make('2048-v0')\n","    n_games = 500\n","    BATCH_NUMBER = 100\n","    BATCH_SIZE = n_games/BATCH_NUMBER\n","\n","    scores_test = []\n","    eps_history_test = []\n","    win_rates_test = []\n","    #counts how many times we have won the game\n","    total_score_test = 0\n","    won = 0\n","    max_value_test = 0\n","    GOAL = 256\n","    max_values_test = []\n","    max_tile_episode_test = [] #to track max_tile in each episode\n","    max_tile_sum_test = 0 #to calculate avg max tile in each batch\n","    avg_max_tile_test = []\n","    \n","    #set epsilon to 0\n","    agent.full_explotation()  \n","\n","    for i in range(n_games):\n","        done = False\n","        #checks if we have won the game, initially set to false\n","        won_mem = False\n","        observation = env.reset()\n","        observation_trans = list(numpy_transformer(observation))\n","        while not done:\n","            observation_ , reward, done, info, action = agent.choose_action_expl(observation_trans,env)\n","            total_score_test += reward\n","            agent.remember(observation_trans, action, reward, observation_, done)\n","            observation_trans = observation_\n","            agent.learn()\n","            # checks if the array has a value, which is bigger than 2048\n","            if max(observation_trans)==GOAL:\n","                won_mem = True\n","            if max(observation_trans)>max_value_test:\n","                max_value_test = max(observation_trans)\n","\n","        max_tile_episode_test.append(max(observation_trans_))\n","        max_tile_sum_test += max(observation_trans_)\n","\n","        eps_history_test.append(agent.epsilon)\n","        #updates the number of games, which we have won\n","        if won_mem == True:\n","            won+=1\n","\n","        \n","\n","        # prints out every 50 episode, how many games we have won from how many episodes and creates a \n","        if (i+1)%BATCH_SIZE == 0:\n","            print(\"You have won \" + str(won) + \" episodes from \" + str(int(i+1)) + \" episodes\")\n","            print(\"The maximal value is: \" + str(max_value_test) + \".\")\n","            scores_test.append(total_score_test/BATCH_SIZE)\n","            win_rates_test.append(won/BATCH_SIZE)\n","            max_values_test.append(max_value_test)\n","            avg_max_tile_test.append(max_tile_sum/BATCH_SIZE)\n","            total_score_test = 0\n","            max_value_test = 0\n","            won = 0\n","            max_tile_sum = 0"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"s3wNWfuO2Kwd"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# get coeffs of linear fit\n","slope, intercept, r_value, p_value, std_err = stats.linregress(x,win_rates_test)\n","\n","# use line_kws to set line label for legend\n","ax = sns.regplot(x=x, y=win_rates_test, color='b', \n"," line_kws={'label':\"y={0:.8f}x+{1:.8f}\".format(slope,intercept)})\n","\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Games won\")\n","plt.title(\"Win Statistic with 256\")\n","\n","# plot legend\n","ax.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"PnNrUZzO2hVM"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# get coeffs of linear fit\n","slope, intercept, r_value, p_value, std_err = stats.linregress(x,scores_test)\n","\n","# use line_kws to set line label for legend\n","ax = sns.regplot(x=x, y=scores_test, color='b', \n"," line_kws={'label':\"y={0:.3f}x+{1:.3f}\".format(slope,intercept)})\n","\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"reward per batch\")\n","plt.title(\"Reward-statistic\")\n","\n","# plot legend\n","ax.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"7c2n3ymE2uL6"},"outputs":[],"source":["plt.scatter(x,max_values_test, label = 'skitscat', color = 'k', s=25 , marker = 'o')\n","plt.xlabel('batches')\n","plt.ylabel('maximal values')\n","plt.title('Maximal value statistic')"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"CboLm_je2vhP"},"outputs":[],"source":["\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","# get coeffs of linear fit\n","slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_max_tile_test)\n","\n","# use line_kws to set line label for legend\n","ax = sns.regplot(x=x, y=avg_max_tile_test, color='b', \n"," line_kws={'label':\"y={0:.3f}x+{1:.3f}\".format(slope,intercept)})\n","\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"avg_max_tile\")\n","plt.title(\"Average max Tile\")\n","\n","# plot legend\n","ax.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"YiyaWCiNNgDR"},"outputs":[],"source":["agent.penalty"]}]}
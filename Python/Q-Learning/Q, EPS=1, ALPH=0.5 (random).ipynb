{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular Q\n",
    "# randomness. Epsilon = 1.\n",
    "# repetitions = 10 --> 10x performance of randomness\n",
    "# except for \"performance test\", there still epsilon 0. let's see what it learned from 5000 episodes randomness\n",
    "# alpha =0.5\n",
    "# training: 5000 episodes\n",
    "# test: 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def maxAction(Q, state):\n",
    "    values = np.array([Q[state,a] for a in range(4)])\n",
    "    action = np.argmax(values)\n",
    "    return action\n",
    "\n",
    "def choseandcheck(Q, a, observation, EPSILON, env, s):\n",
    "    rand = np.random.random()\n",
    "    if rand < (1-EPSILON):\n",
    "        a = maxAction(Q,s)\n",
    "    else:\n",
    "        a = np.random.randint(0,4)\n",
    "   # print(env.step(a), a)\n",
    "    new_state, reward, done, _ = env.step(a)\n",
    "    observation_ = numpy_transformer_light(new_state)\n",
    "    #print(observation_, '\\n\\n', observation)\n",
    "    while observation==observation_:\n",
    "        #print(\"REACHED)\")\n",
    "        Q[s,a] -= 1\n",
    "        rand = np.random.random()\n",
    "        if rand < (1-EPSILON):\n",
    "            a = maxAction(Q,s)\n",
    "        else:\n",
    "            a = np.random.randint(0,4)\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(a)\n",
    "        observation_ = numpy_transformer_light(new_state)\n",
    "        \n",
    "    return a, observation_, reward, done\n",
    "\n",
    "\n",
    "def numpy_transformer(matrix):\n",
    "    lst = []\n",
    "    for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[0])):\n",
    "                    lst.append(int(matrix[i][j]))\n",
    "    return tuple(lst)\n",
    "\n",
    "def numpy_transformer_light(matrix):\n",
    "    lst = []\n",
    "    for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[0])):\n",
    "                    lst.append(int(matrix[i][j]))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def get_max(state):\n",
    "    max_value = max(state)\n",
    "    return max_value\n",
    "\n",
    "def plotLearning(x, y, x_label, y_label,title):\n",
    "    plt.scatter(x,y, label = 'skitscat', color = 'k', s=25 , marker = 'o')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\matte\\Anaconda3\\lib\\site-packages\\gym_2048\\env.py:120: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  board[tile_locs] = tiles\nEpisode: 5\nEpisode: 10\nEpisode: 15\nEpisode: 20\nEpisode: 25\nEpisode: 30\nEpisode: 35\nEpisode: 40\nEpisode: 45\nEpisode: 50\nEpisode: 55\nEpisode: 60\nEpisode: 65\nEpisode: 70\nEpisode: 75\nEpisode: 80\nEpisode: 85\nEpisode: 90\nEpisode: 95\nEpisode: 100\nEpisode: 105\nEpisode: 110\nEpisode: 115\nEpisode: 120\nEpisode: 125\nEpisode: 130\nEpisode: 135\nEpisode: 140\nEpisode: 145\nEpisode: 150\nEpisode: 155\nEpisode: 160\nEpisode: 165\nEpisode: 170\nEpisode: 175\nEpisode: 180\nEpisode: 185\nEpisode: 190\nEpisode: 195\nEpisode: 200\nEpisode: 205\nEpisode: 210\nEpisode: 215\nEpisode: 220\nEpisode: 225\nEpisode: 230\nEpisode: 235\nEpisode: 240\nEpisode: 245\nEpisode: 250\nEpisode: 255\nEpisode: 260\nEpisode: 265\nEpisode: 270\nEpisode: 275\nEpisode: 280\nEpisode: 285\nEpisode: 290\nEpisode: 295\nEpisode: 300\nEpisode: 305\nEpisode: 310\nEpisode: 315\nEpisode: 320\nEpisode: 325\nEpisode: 330\nEpisode: 335\nEpisode: 340\nEpisode: 345\nEpisode: 350\nEpisode: 355\nEpisode: 360\nEpisode: 365\nEpisode: 370\nEpisode: 375\nEpisode: 380\nEpisode: 385\nEpisode: 390\nEpisode: 395\nEpisode: 400\nEpisode: 405\nEpisode: 410\nEpisode: 415\nEpisode: 420\nEpisode: 425\nEpisode: 430\nEpisode: 435\nEpisode: 440\nEpisode: 445\nEpisode: 450\nEpisode: 455\nEpisode: 460\nEpisode: 465\nEpisode: 470\nEpisode: 475\nEpisode: 480\nEpisode: 485\nEpisode: 490\nEpisode: 495\n"
    }
   ],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import gym_2048\n",
    "import random\n",
    "\n",
    "#define number of rounds\n",
    "repetitions = 10\n",
    "#define episodes per round, use multiple of 100\n",
    "numGames = 500\n",
    "round = 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('2048-v0')\n",
    "\n",
    "    #model hyperparameters\n",
    "    ALPHA = 0.5\n",
    "    GAMMA = 0.99\n",
    "    EPSILON = 1.0\n",
    "    #this variable counts how many times we have won.\n",
    "    won = 0\n",
    "    #value we want to reach due to memory restrictions.\n",
    "    GOAL = 256\n",
    "    max_value = 0\n",
    "    num_won = []\n",
    "    max_values = []\n",
    "    avg_values = []\n",
    "    max_total = 0\n",
    "\n",
    "    #construct state space\n",
    " \n",
    "    states = []\n",
    "    Q = {}\n",
    "\n",
    "\n",
    "\n",
    "    BATCH_SIZE = int(numGames/100)\n",
    "    totalRewards = []\n",
    "    for i in range(numGames):\n",
    "        game_won = False\n",
    "        observation = numpy_transformer_light(env.reset())\n",
    "        s = tuple(observation)\n",
    "        #check if observation isn't already in states\n",
    "        if observation not in states:\n",
    "            states.append(observation)\n",
    "            for a in range(4):\n",
    "                #we changed the state to a to avoid a TypeError,\n",
    "                #because lists aren't hashable.S\n",
    "                Q[tuple(s),a] = 0\n",
    "        rand = np.random.random()\n",
    "\n",
    "        done = False\n",
    "        epRewards = 0\n",
    "        while not done:      \n",
    "            # choice and validity check to avoid dead ends (loops)\n",
    "            a, observation_, reward, done = choseandcheck(Q, a, observation, EPSILON, env, s)\n",
    "\n",
    "            s_ = tuple(observation_)\n",
    "            if observation_ not in states:\n",
    "                states.append(observation_)\n",
    "                for a in range(4):\n",
    "                    #we changed the state to a to avoid a TypeError,\n",
    "                    #because lists aren't hashable.\n",
    "                    Q[tuple(s_),a] = 0\n",
    "                    \n",
    "\n",
    "            rand = np.random.random()\n",
    "            if rand < (1-EPSILON):\n",
    "                a_ = maxAction(Q,s_)\n",
    "            else: \n",
    "                a_ = np.random.randint(0,4)\n",
    "            epRewards += reward\n",
    "            Q[s,a] = Q[s,a] + ALPHA*(reward + GAMMA*Q[s_,maxAction(Q,s_)] - Q[s,a])\n",
    "            \n",
    "            # no more needed. that does not check for validity of move. we incorporated thi in choseandcheck\n",
    "            # s,a = s_,a_\n",
    "\n",
    "            #checks if the GOAL is reached. Sets the done to True to avoid KeyError (a higher state can be reached\n",
    "            # but we don't want to reach it, because we don't have in in our state_space.)\n",
    "            if get_max(observation_)==GOAL:\n",
    "                game_won = True\n",
    "            \n",
    "            if get_max(observation_)>max_value:\n",
    "                max_value = get_max(observation_)\n",
    "                \n",
    "            observation=observation_\n",
    "        \n",
    "        \n",
    "        if game_won == True:\n",
    "            won+= 1\n",
    "\n",
    "        #if EPSILON > 0.01:\n",
    "        #    EPSILON -= 1/(numGames)  \n",
    "        #else:\n",
    "        #    EPSILON = 0.01\n",
    "        totalRewards.append(epRewards)\n",
    "        max_total += get_max(observation)\n",
    "        \n",
    "        if (i+1)%(BATCH_SIZE) == 0:\n",
    "            new = won/(BATCH_SIZE)\n",
    "            num_won.append(new)\n",
    "            max_values.append(max_value)\n",
    "            average = max_total/BATCH_SIZE\n",
    "            avg_values.append(average)\n",
    "            won = 0\n",
    "            max_value = 0\n",
    "            max_total = 0\n",
    "            print(\"Episode: \" + str(i+1))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [j for j in range(1,101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plotLearning(x,num_won,'Batch','Won games in percentage', 'Win-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=num_won, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_performance_test = intercept + 50*slope\n",
    "print(\"Average performance is \", avg_performance_test)\n",
    "\n",
    "plotLearning(x,max_values,'Batch','Max_values', 'Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,max_values)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=max_values, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_performance_test = intercept + 50*slope\n",
    "print(\"Average performance is \", avg_performance_test)\n",
    "\n",
    "\n",
    "plotLearning(x,avg_values,'Batch','Avg_values', 'Average-Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_values, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "print(\"round : \", round)\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_performance_test = intercept + 50*slope\n",
    "print(\"Average performance is \", avg_performance_test)\n",
    "\n",
    "\n",
    "\n",
    "print(len(totalRewards))\n",
    "\n",
    "x = [j for j in range(1,len(totalRewards)+1)]\n",
    "\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,totalRewards)\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.title(\"rewards of random agent\")\n",
    "\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=totalRewards, color='b', \n",
    " line_kws={'label':\"y={0:.4f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "avg_performance_test = intercept + numGames/2*slope\n",
    "print(\"Average performance is \", avg_performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to add how many repetitive rounds...enter number of repetitions\n",
    "\n",
    "while round < repetitions:\n",
    "    round += 1\n",
    "    if __name__ == '__main__':\n",
    "        env = gym.make('2048-v0')\n",
    "\n",
    "        #model hyperparameters (commetn what doesn need to be rewritten)\n",
    "        #ALPHA = 0.1\n",
    "        #GAMMA = 0.99\n",
    "        #As already some knowledge, less randomness. still decreasing. reduce start             with each repetition\n",
    "        EPSILON = 1   #/round\n",
    "        #this variable counts how many times we have won.\n",
    "        won = 0\n",
    "        #value we want to reach due to memory restrictions.\n",
    "        #GOAL = 512\n",
    "        max_value = 0\n",
    "        #num_won = []\n",
    "        #max_values = []\n",
    "        #avg_values = []\n",
    "        max_total = 0\n",
    "\n",
    "        #construct state space\n",
    "    \n",
    "        #states = []\n",
    "        #Q = {}\n",
    "\n",
    "        #numGames = 5000\n",
    "        BATCH_SIZE = int(numGames/100)\n",
    "        \n",
    "        for i in range(numGames):\n",
    "            game_won = False\n",
    "            observation = numpy_transformer_light(env.reset())\n",
    "            s = tuple(observation)\n",
    "            #check if observation isn't already in states\n",
    "            if observation not in states:\n",
    "                states.append(observation)\n",
    "                for a in range(4):\n",
    "                    #we changed the state to a to avoid a TypeError,\n",
    "                    #because lists aren't hashable.S\n",
    "                    Q[tuple(s),a] = 0\n",
    "            rand = np.random.random()\n",
    "\n",
    "            done = False\n",
    "            epRewards = 0\n",
    "            while not done:      \n",
    "                # choice and validity check to avoid dead ends (loops)\n",
    "                a, observation_, reward, done = choseandcheck(Q, a, observation, EPSILON, env, s)\n",
    "\n",
    "                s_ = tuple(observation_)\n",
    "                if observation_ not in states:\n",
    "                    states.append(observation_)\n",
    "                    for a in range(4):\n",
    "                        #we changed the state to a to avoid a TypeError,\n",
    "                        #because lists aren't hashable.\n",
    "                        Q[tuple(s_),a] = 0\n",
    "                        \n",
    "\n",
    "                rand = np.random.random()\n",
    "                if rand < (1-EPSILON):\n",
    "                    a_ = maxAction(Q,s_)\n",
    "                else: \n",
    "                    a_ = np.random.randint(0,4)\n",
    "                epRewards += reward\n",
    "                Q[s,a] = Q[s,a] + ALPHA*(reward + GAMMA*Q[s_,maxAction(Q,s_)] - Q[s,a])\n",
    "                \n",
    "                # no more needed. that does not check for validity of move. we incorporated thi in choseandcheck\n",
    "                # s,a = s_,a_\n",
    "\n",
    "                #checks if the GOAL is reached. Sets the done to True to avoid KeyError (a higher state can be reached\n",
    "                # but we don't want to reach it, because we don't have in in our state_space.)\n",
    "                if get_max(observation_)==GOAL:\n",
    "                    game_won = True\n",
    "                \n",
    "                if get_max(observation_)>max_value:\n",
    "                    max_value = get_max(observation_)\n",
    "                    \n",
    "                observation=observation_\n",
    "            \n",
    "            \n",
    "            if game_won == True:\n",
    "                won+= 1\n",
    "\n",
    "            #if EPSILON > 0.01:\n",
    "            #    EPSILON -= 1/(round*numGames)  \n",
    "           # else:\n",
    "            #    EPSILON = 0.01\n",
    "            totalRewards.append(epRewards)\n",
    "            max_total += get_max(observation)\n",
    "            \n",
    "            if (i+1)%(BATCH_SIZE) == 0:\n",
    "                new = won/(BATCH_SIZE)\n",
    "                num_won.append(new)\n",
    "                max_values.append(max_value)\n",
    "                average = max_total/BATCH_SIZE\n",
    "                avg_values.append(average)\n",
    "                won = 0\n",
    "                max_value = 0\n",
    "                max_total = 0\n",
    "                print(\"Episode: \" + str(i+1))\n",
    "\n",
    "\n",
    "\n",
    "    x = [j for j in range(1,101)]\n",
    "    lower_bound= (round-1)*100\n",
    "    upper_bound= round*100\n",
    "\n",
    "\n",
    "\n",
    "    print(\"summary stuff from round \", round,\" below\")\n",
    "    plotLearning(x,num_won[lower_bound:upper_bound],'Batch','Won games in percentage', 'Win-Statistic')\n",
    "    # get coeffs of linear fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won[lower_bound:upper_bound])\n",
    "\n",
    "    # use line_kws to set line label for legend\n",
    "    ax = sns.regplot(x=x, y=num_won[lower_bound:upper_bound], color='b', \n",
    "    line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "    # plot legend\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "    print(\"round : \", round)\n",
    "\n",
    "    avg_performance_test = intercept + 50*slope\n",
    "    print(\"Average performance is \", avg_performance_test)# plot legend\n",
    "\n",
    "\n",
    "    plotLearning(x,max_values[lower_bound:upper_bound],'Batch','Max_values', 'Max-Statistic')\n",
    "    # get coeffs of linear fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,max_values[lower_bound:upper_bound])\n",
    "\n",
    "    # use line_kws to set line label for legend\n",
    "    ax = sns.regplot(x=x, y=max_values[lower_bound:upper_bound], color='b', \n",
    "    line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "    # plot legend\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    avg_performance_test = intercept + 50*slope\n",
    "    print(\"Average performance is \", avg_performance_test)# plot legend\n",
    "\n",
    "\n",
    "\n",
    "    plotLearning(x,avg_values[lower_bound:upper_bound],'Batch','Avg_values', 'Average-Max-Statistic')\n",
    "    # get coeffs of linear fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values[lower_bound:upper_bound])\n",
    "\n",
    "    # use line_kws to set line label for legend\n",
    "    ax = sns.regplot(x=x, y=avg_values[lower_bound:upper_bound], color='b', \n",
    "    line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "    # plot legend\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    avg_performance_test = intercept + 50*slope\n",
    "    print(\"Average performance is \", avg_performance_test)# plot legend\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound= repetitions*numGames +1\n",
    "x = [j for j in range(1,upper_bound)]\n",
    "    \n",
    "    print(len(totalRewards))\n",
    "\n",
    "\n",
    "    plotLearning(x,avg_values[:upper_bound],'Episode','totalRewards', 'Reward')\n",
    "    # get coeffs of linear fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,totalRewards[:upper_bound])\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"rewards\")\n",
    "    plt.title(\"random agent overall training\")\n",
    "\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "    ax = sns.regplot(x=x, y=totalRewards[:upper_bound], color='b', \n",
    "    line_kws={'label':\"y={0:.4f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "avg_performance_test = intercept + repetitions*numGames/2*slope\n",
    "print(\"Average overall performance is \", avg_performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the performance\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('2048-v0')\n",
    "\n",
    "    #model hyperparameters\n",
    "    #ALPHA = 0.5\n",
    "    #GAMMA = 0.99\n",
    "    EPSILON = 0\n",
    "    #this variable counts how many times we have won.\n",
    "    won = 0\n",
    "    #value we want to reach due to memory restrictions.\n",
    "    #GOAL = 256\n",
    "    max_value_test = 0\n",
    "    num_won_test = []\n",
    "    max_values_test = []\n",
    "    avg_values_test = []\n",
    "    avg_reward_test = []\n",
    "    max_total_test = 0\n",
    "    total_reward_test = 0\n",
    "\n",
    "    #do not overwrite spaces\n",
    " \n",
    "    #states = []\n",
    "    #Q = {}\n",
    "\n",
    "    numGames_test = 500\n",
    "    BATCH_SIZE = int(numGames_test/100)\n",
    "    for i in range(numGames_test):\n",
    "        game_won = False\n",
    "        observation = numpy_transformer_light(env.reset())\n",
    "        s = tuple(observation)\n",
    "        #check if observation isn't already in states\n",
    "        if observation not in states:\n",
    "            states.append(observation)\n",
    "            for a in range(4):\n",
    "                #we changed the state to a to avoid a TypeError,\n",
    "                #because lists aren't hashable.S\n",
    "                Q[tuple(s),a] = 0\n",
    "        rand = np.random.random()\n",
    "\n",
    "        done = False\n",
    "        epRewards = 0\n",
    "        while not done:      \n",
    "            # choice and validity check to avoid dead ends (loops)\n",
    "            a, observation_, reward, done = choseandcheck(Q, a, observation, EPSILON, env, s)\n",
    "\n",
    "            s_ = tuple(observation_)\n",
    "            if observation_ not in states:\n",
    "                states.append(observation_)\n",
    "                for a in range(4):\n",
    "                    #we changed the state to a to avoid a TypeError,\n",
    "                    #because lists aren't hashable.\n",
    "                    Q[tuple(s_),a] = 0\n",
    "                    \n",
    "\n",
    "            rand = np.random.random()\n",
    "            #this part no more needed actually but \"too lazy\" to adapt. (epsilon anyway = 0)\n",
    "            if rand < (1-EPSILON):\n",
    "                a_ = maxAction(Q,s_)\n",
    "            else: \n",
    "                a_ = np.random.randint(0,4)\n",
    "            epRewards += reward\n",
    "            Q[s,a] = Q[s,a] + ALPHA*(reward + GAMMA*Q[s_,maxAction(Q,s_)] - Q[s,a])\n",
    "            \n",
    "            # no more needed. that does not check for validity of move. we incorporated thi in choseandcheck\n",
    "            # s,a = s_,a_\n",
    "\n",
    "            #checks if the GOAL is reached. Sets the done to True to avoid KeyError (a higher state can be reached\n",
    "            # but we don't want to reach it, because we don't have in in our state_space.)\n",
    "            if get_max(observation_)==GOAL:\n",
    "                game_won = True\n",
    "            \n",
    "            if get_max(observation_)>max_value_test:\n",
    "                max_value_test = get_max(observation_)\n",
    "                \n",
    "            observation=observation_\n",
    "        \n",
    "        total_reward_test += epRewards\n",
    "        if game_won == True:\n",
    "            won+= 1\n",
    "\n",
    "        #if EPSILON > 0.0002:\n",
    "        #    EPSILON -= 2/(numGames_test)  \n",
    "        #else:\n",
    "        #    EPSILON = 0.0002\n",
    "        max_total_test += get_max(observation)\n",
    "        \n",
    "        if (i+1)%(numGames_test/100) == 0:\n",
    "            new = won/(BATCH_SIZE)\n",
    "            num_won_test.append(new)\n",
    "            max_values_test.append(max_value_test)\n",
    "            average = max_total_test/BATCH_SIZE\n",
    "            avg_values_test.append(average)\n",
    "            avg_reward_test.append(total_reward_test/BATCH_SIZE)\n",
    "            won = 0\n",
    "            max_value_test = 0\n",
    "            max_total_test = 0\n",
    "            average = 0\n",
    "            total_reward_test = 0\n",
    "            print(\"Episode: \" + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [j for j in range(1,101)]\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won_test)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=num_won_test, color='b', \n",
    " line_kws={'label':\"y={0:.3f}x+{1:.3f}\".format(slope,intercept)})\n",
    "\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Games won\")\n",
    "plt.title(\"Win Statistic of test\")\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_performance_test = intercept + 50*slope\n",
    "print(\"Average performance is \", avg_performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_reward_test)\n",
    "\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Average of Rewards\")\n",
    "plt.title(\"Average of Rewards of Q learning after random agent\")\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_reward_test, color='b', \n",
    " line_kws={'label':\"y={0:.2f}x+{1:.2f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_performance_test = intercept + 50*slope\n",
    "print(\"Average performance is \", avg_performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values_test)\n",
    "\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Average of Maximas\")\n",
    "plt.title(\"Average of maximal values of test\")\n",
    "\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_values_test, color='b', \n",
    " line_kws={'label':\"y={0:.4f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_performance_test = intercept + 50*slope\n",
    "print(\"Average performance is \", avg_performance_test)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
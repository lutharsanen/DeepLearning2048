{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def maxAction(Q, state):\n",
    "    values = np.array([Q[state,a] for a in range(4)])\n",
    "    action = np.argmax(values)\n",
    "    return action\n",
    "\n",
    "def choseandcheck(Q, a, observation, EPSILON, env, s):\n",
    "    rand = np.random.random()\n",
    "    if rand < (1-EPSILON):\n",
    "        a = maxAction(Q,s)\n",
    "    else:\n",
    "        a = np.random.randint(0,4)\n",
    "   # print(env.step(a), a)\n",
    "    new_state, reward, done, _ = env.step(a)\n",
    "    observation_ = numpy_transformer_light(new_state)\n",
    "    #print(observation_, '\\n\\n', observation)\n",
    "    while observation==observation_:\n",
    "        #print(\"REACHED)\")\n",
    "        Q[s,a] -= 1\n",
    "        rand = np.random.random()\n",
    "        if rand < (1-EPSILON):\n",
    "            a = maxAction(Q,s)\n",
    "        else:\n",
    "            a = np.random.randint(0,4)\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(a)\n",
    "        observation_ = numpy_transformer_light(new_state)\n",
    "        \n",
    "    return a, observation_, reward, done\n",
    "\n",
    "\n",
    "def numpy_transformer(matrix):\n",
    "    lst = []\n",
    "    for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[0])):\n",
    "                    lst.append(int(matrix[i][j]))\n",
    "    return tuple(lst)\n",
    "\n",
    "def numpy_transformer_light(matrix):\n",
    "    lst = []\n",
    "    for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[0])):\n",
    "                    lst.append(int(matrix[i][j]))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def get_max(state):\n",
    "    max_value = max(state)\n",
    "    return max_value\n",
    "\n",
    "def plotLearning(x, y, x_label, y_label,title):\n",
    "    plt.scatter(x,y, label = 'skitscat', color = 'k', s=25 , marker = 'o')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import gym_2048\n",
    "import random\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('2048-v0')\n",
    "\n",
    "    #model hyperparameters\n",
    "    ALPHA = 0.1\n",
    "    GAMMA = 0.99\n",
    "    EPSILON = 1.0\n",
    "    #this variable counts how many times we have won.\n",
    "    won = 0\n",
    "    #value we want to reach due to memory restrictions.\n",
    "    GOAL = 512\n",
    "    max_value = 0\n",
    "    num_won = []\n",
    "    max_values = []\n",
    "    avg_values = []\n",
    "    max_total = 0\n",
    "\n",
    "    #construct state space\n",
    " \n",
    "    states = []\n",
    "    Q = {}\n",
    "\n",
    "    numGames = 500\n",
    "    BATCH_SIZE = int(numGames/100)\n",
    "    totalRewards = []\n",
    "    for i in range(numGames):\n",
    "        game_won = False\n",
    "        observation = numpy_transformer_light(env.reset())\n",
    "        s = tuple(observation)\n",
    "        #check if observation isn't already in states\n",
    "        if observation not in states:\n",
    "            states.append(observation)\n",
    "            for a in range(4):\n",
    "                #we changed the state to a to avoid a TypeError,\n",
    "                #because lists aren't hashable.S\n",
    "                Q[tuple(s),a] = 0\n",
    "        rand = np.random.random()\n",
    "\n",
    "        done = False\n",
    "        epRewards = 0\n",
    "        while not done:      \n",
    "            # choice and validity check to avoid dead ends (loops)\n",
    "            a, observation_, reward, done = choseandcheck(Q, a, observation, EPSILON, env, s)\n",
    "\n",
    "            s_ = tuple(observation_)\n",
    "            if observation_ not in states:\n",
    "                states.append(observation_)\n",
    "                for a in range(4):\n",
    "                    #we changed the state to a to avoid a TypeError,\n",
    "                    #because lists aren't hashable.\n",
    "                    Q[tuple(s_),a] = 0\n",
    "                    \n",
    "\n",
    "            rand = np.random.random()\n",
    "            if rand < (1-EPSILON):\n",
    "                a_ = maxAction(Q,s_)\n",
    "            else: \n",
    "                a_ = np.random.randint(0,4)\n",
    "            epRewards += reward\n",
    "            Q[s,a] = Q[s,a] + ALPHA*(reward + GAMMA*Q[s_,a_] - Q[s,a])\n",
    "            \n",
    "            # no more needed. that does not check for validity of move. we incorporated thi in choseandcheck\n",
    "            # s,a = s_,a_\n",
    "\n",
    "            #checks if the GOAL is reached. Sets the done to True to avoid KeyError (a higher state can be reached\n",
    "            # but we don't want to reach it, because we don't have in in our state_space.)\n",
    "            if get_max(observation_)==GOAL:\n",
    "                game_won = True\n",
    "            \n",
    "            if get_max(observation_)>max_value:\n",
    "                max_value = get_max(observation_)\n",
    "                \n",
    "            observation=observation_\n",
    "        \n",
    "        \n",
    "        if game_won == True:\n",
    "            won+= 1\n",
    "\n",
    "        if EPSILON > 0.0002:\n",
    "            EPSILON -= 2/(numGames)  \n",
    "        else:\n",
    "            EPSILON = 0.0002\n",
    "        totalRewards.append(epRewards)\n",
    "        max_total += get_max(observation)\n",
    "        \n",
    "        if (i+1)%(BATCH_SIZE) == 0:\n",
    "            new = won/(BATCH_SIZE)\n",
    "            num_won.append(new)\n",
    "            max_values.append(max_value)\n",
    "            average = max_total/BATCH_SIZE\n",
    "            avg_values.append(average)\n",
    "            won = 0\n",
    "            max_value = 0\n",
    "            max_total = 0\n",
    "            print(\"Episode: \" + str(i+1))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [j for j in range(1,101)]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plotLearning(x,num_won,'Batch','Won games in percentage', 'Win-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=num_won, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plotLearning(x,max_values,'Batch','Max_values', 'Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,max_values)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=max_values, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plotLearning(x,avg_values,'Batch','Avg_values', 'Average-Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_values, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 2\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('2048-v0')\n",
    "\n",
    "    #model hyperparameters (commetn what doesn need to be rewritten)\n",
    "    #ALPHA = 0.1\n",
    "    GAMMA = 0.99\n",
    "    #As already some knowledge, less randomness. still decreasing. reduce start with each repetition\n",
    "    EPSILON = 1/round\n",
    "    #this variable counts how many times we have won.\n",
    "    won = 0\n",
    "    #value we want to reach due to memory restrictions.\n",
    "    #GOAL = 512\n",
    "    max_value = 0\n",
    "    #num_won = []\n",
    "    #max_values = []\n",
    "    #avg_values = []\n",
    "    max_total = 0\n",
    "\n",
    "    #construct state space\n",
    " \n",
    "    #states = []\n",
    "    #Q = {}\n",
    "\n",
    "    #numGames = 5000\n",
    "    BATCH_SIZE = int(numGames/100)\n",
    "    \n",
    "    for i in range(numGames):\n",
    "        game_won = False\n",
    "        observation = numpy_transformer_light(env.reset())\n",
    "        s = tuple(observation)\n",
    "        #check if observation isn't already in states\n",
    "        if observation not in states:\n",
    "            states.append(observation)\n",
    "            for a in range(4):\n",
    "                #we changed the state to a to avoid a TypeError,\n",
    "                #because lists aren't hashable.S\n",
    "                Q[tuple(s),a] = 0\n",
    "        rand = np.random.random()\n",
    "\n",
    "        done = False\n",
    "        epRewards = 0\n",
    "        while not done:      \n",
    "            # choice and validity check to avoid dead ends (loops)\n",
    "            a, observation_, reward, done = choseandcheck(Q, a, observation, EPSILON, env, s)\n",
    "\n",
    "            s_ = tuple(observation_)\n",
    "            if observation_ not in states:\n",
    "                states.append(observation_)\n",
    "                for a in range(4):\n",
    "                    #we changed the state to a to avoid a TypeError,\n",
    "                    #because lists aren't hashable.\n",
    "                    Q[tuple(s_),a] = 0\n",
    "                    \n",
    "\n",
    "            rand = np.random.random()\n",
    "            if rand < (1-EPSILON):\n",
    "                a_ = maxAction(Q,s_)\n",
    "            else: \n",
    "                a_ = np.random.randint(0,4)\n",
    "            epRewards += reward\n",
    "            Q[s,a] = Q[s,a] + ALPHA*(reward + GAMMA*Q[s_,a_] - Q[s,a])\n",
    "            \n",
    "            # no more needed. that does not check for validity of move. we incorporated thi in choseandcheck\n",
    "            # s,a = s_,a_\n",
    "\n",
    "            #checks if the GOAL is reached. Sets the done to True to avoid KeyError (a higher state can be reached\n",
    "            # but we don't want to reach it, because we don't have in in our state_space.)\n",
    "            if get_max(observation_)==GOAL:\n",
    "                game_won = True\n",
    "            \n",
    "            if get_max(observation_)>max_value:\n",
    "                max_value = get_max(observation_)\n",
    "                \n",
    "            observation=observation_\n",
    "        \n",
    "        \n",
    "        if game_won == True:\n",
    "            won+= 1\n",
    "\n",
    "        if EPSILON > 0.0002:\n",
    "            EPSILON -= 2/(numGames)  \n",
    "        else:\n",
    "            EPSILON = 0.0002\n",
    "        totalRewards.append(epRewards)\n",
    "        max_total += get_max(observation)\n",
    "        \n",
    "        if (i+1)%(BATCH_SIZE) == 0:\n",
    "            new = won/(BATCH_SIZE)\n",
    "            num_won.append(new)\n",
    "            max_values.append(max_value)\n",
    "            average = max_total/BATCH_SIZE\n",
    "            avg_values.append(average)\n",
    "            won = 0\n",
    "            max_value = 0\n",
    "            max_total = 0\n",
    "            print(\"Episode: \" + str(i+1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [j for j in range(1,101)]\n",
    "lower_bound= (round-1)*100\n",
    "upper_bound= round*100"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearning(x,num_won[lower_bound:],'Batch','Won games in percentage', 'Win-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won[lower_bound:])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=num_won[lower_bound:], color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plotLearning(x,max_values[lower_bound:],'Batch','Max_values', 'Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,max_values[lower_bound:])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=max_values[lower_bound:], color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plotLearning(x,avg_values[lower_bound:],'Batch','Avg_values', 'Average-Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values[lower_bound:])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_values[lower_bound:], color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "round += 1\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('2048-v0')\n",
    "\n",
    "    #model hyperparameters (commetn what doesn need to be rewritten)\n",
    "    #ALPHA = 0.1\n",
    "    GAMMA = 0.99\n",
    "    #As already some knowledge, less randomness. still decreasing. reduce start with each repetition\n",
    "    EPSILON = 1/round\n",
    "    #this variable counts how many times we have won.\n",
    "    won = 0\n",
    "    #value we want to reach due to memory restrictions.\n",
    "    #GOAL = 512\n",
    "    max_value = 0\n",
    "    #num_won = []\n",
    "    #max_values = []\n",
    "    #avg_values = []\n",
    "    max_total = 0\n",
    "\n",
    "    #construct state space\n",
    " \n",
    "    #states = []\n",
    "    #Q = {}\n",
    "\n",
    "    #numGames = 5000\n",
    "    BATCH_SIZE = int(numGames/100)\n",
    "    \n",
    "    for i in range(numGames):\n",
    "        game_won = False\n",
    "        observation = numpy_transformer_light(env.reset())\n",
    "        s = tuple(observation)\n",
    "        #check if observation isn't already in states\n",
    "        if observation not in states:\n",
    "            states.append(observation)\n",
    "            for a in range(4):\n",
    "                #we changed the state to a to avoid a TypeError,\n",
    "                #because lists aren't hashable.S\n",
    "                Q[tuple(s),a] = 0\n",
    "        rand = np.random.random()\n",
    "\n",
    "        done = False\n",
    "        epRewards = 0\n",
    "        while not done:      \n",
    "            # choice and validity check to avoid dead ends (loops)\n",
    "            a, observation_, reward, done = choseandcheck(Q, a, observation, EPSILON, env, s)\n",
    "\n",
    "            s_ = tuple(observation_)\n",
    "            if observation_ not in states:\n",
    "                states.append(observation_)\n",
    "                for a in range(4):\n",
    "                    #we changed the state to a to avoid a TypeError,\n",
    "                    #because lists aren't hashable.\n",
    "                    Q[tuple(s_),a] = 0\n",
    "                    \n",
    "\n",
    "            rand = np.random.random()\n",
    "            if rand < (1-EPSILON):\n",
    "                a_ = maxAction(Q,s_)\n",
    "            else: \n",
    "                a_ = np.random.randint(0,4)\n",
    "            epRewards += reward\n",
    "            Q[s,a] = Q[s,a] + ALPHA*(reward + GAMMA*Q[s_,a_] - Q[s,a])\n",
    "            \n",
    "            # no more needed. that does not check for validity of move. we incorporated thi in choseandcheck\n",
    "            # s,a = s_,a_\n",
    "\n",
    "            #checks if the GOAL is reached. Sets the done to True to avoid KeyError (a higher state can be reached\n",
    "            # but we don't want to reach it, because we don't have in in our state_space.)\n",
    "            if get_max(observation_)==GOAL:\n",
    "                game_won = True\n",
    "            \n",
    "            if get_max(observation_)>max_value:\n",
    "                max_value = get_max(observation_)\n",
    "                \n",
    "            observation=observation_\n",
    "        \n",
    "        \n",
    "        if game_won == True:\n",
    "            won+= 1\n",
    "\n",
    "        if EPSILON > 0.0002:\n",
    "            EPSILON -= 2/(numGames)  \n",
    "        else:\n",
    "            EPSILON = 0.0002\n",
    "        totalRewards.append(epRewards)\n",
    "        max_total += get_max(observation)\n",
    "        \n",
    "        if (i+1)%(BATCH_SIZE) == 0:\n",
    "            new = won/(BATCH_SIZE)\n",
    "            num_won.append(new)\n",
    "            max_values.append(max_value)\n",
    "            average = max_total/BATCH_SIZE\n",
    "            avg_values.append(average)\n",
    "            won = 0\n",
    "            max_value = 0\n",
    "            max_total = 0\n",
    "            print(\"Episode: \" + str(i+1))    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [j for j in range(1,101)]\n",
    "lower_bound= (round-1)*100\n",
    "upper_bound= round*100"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearning(x,num_won[lower_bound:],'Batch','Won games in percentage', 'Win-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won[lower_bound:])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=num_won[lower_bound:], color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plotLearning(x,max_values[lower_bound:],'Batch','Max_values', 'Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,max_values[lower_bound:])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=max_values[lower_bound:], color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plotLearning(x,avg_values[lower_bound:],'Batch','Avg_values', 'Average-Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values[lower_bound:])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_values[lower_bound:], color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to visualize overall progress\n",
    "overall_episodes = round*numGames\n",
    "print(overall_episodes)\n",
    "boundary_overall =overall_episodes+1\n",
    "x = [j for j in range(1, boundary_overall)]\n",
    "\n",
    "plotLearning(x,num_won,'Batch','Won games in percentage', 'Win-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,num_won)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=num_won, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plotLearning(x,max_values,'Batch','Max_values', 'Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,max_values)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=max_values, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plotLearning(x,avg_values,'Batch','Avg_values', 'Average-Max-Statistic')\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,avg_values)\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=x, y=avg_values, color='b', \n",
    " line_kws={'label':\"y={0:.7f}x+{1:.7f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ]
}